# 1.wordcount的实现过程
### Map 阶段：
1. 按行读取要统计的文本文件；
2. 按空格拆分每行的单词
3. 将单词转换成 kv 键值对，格式为（单词，1）
4. 将所有的 kv 键值对中的单词按照单词首字母分区，比如分两个区，那么分区1（a-p），分区2（q-z）
### Reduce 阶段：
1. 每个 ReduceTask 拉取上一阶段所有 MapTask 的输出，按照 key 汇总
2. 对相同的 key 的 value（value 都是 1） 求和
# 2.MapReduce 与 Spark 的区别
- MapReduce 是一个分布式运算程序的编程框架。核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并行运行在一个Hadoop集群上。
- Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。
- Spark 和 Hadoop 的根本差异是多个作业之间的数据通信问题 :
  - Spark多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘。
  - Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互。
  - Spark 的弹性分布式数据集（Resilient Distributed Datasets），提供了比 MapReduce 丰富的模型，可以快速在内存中对数据集进行多次迭代，MapReduce 的设计初衷并不是为了满足循环迭代式数据流处理。
  - Spark 是基于内存的，由于内存的限制，可能会由于内存资源不够导致Job执行失败，数据量大时 MapReduce 是一个更好的选择。
# 3.Spark 在 client 与在 cluster 运行的区别
- Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试。
- Cluster模式将用于监控和调度的Driver模块启动在Yarn集群资源中执行。一般应用于实际生产环境。
# 4.相同的 SQL 在 Hive SQL 与 Spark SQL 的实现中，为什么 Spark 比 Hadoop 快
- Hive 的底层时 MapReduce，所以比 Spark 慢
# 5.UDF 是什么
- UDF（User-Defined Functions）是用户定义的 hive 函数。
- UDF：one to one，一进一出，如：upper、substr函数
- UDAF：many to one，多进一出，如：sum、min。
- UDTF：one to many ，一进多出。如：alteral view 与 explode
# 6.设计 HBase 表需要注意的点
- Hbase 中一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计 rowkey的主要目的 ，就是让数据均匀的分布于所有的 region 中，在一定程度上防止数据倾斜。常用方法：
  - 生成随机数、hash、散列值。比如：原本 rowKey 为1001的哈希后变成:dd01903921ea24941c26a48f2cec24e0bb0e8cc7
  - 字符串反转:20170524000001转成10000042507102，20170524000002转成20000042507102
  - 字符串拼接:20170524000001_a12e，20170524000001_93i7
# 7.HBase的 Hlog
### Hlog简介
- Hlog是Hbase实现WAL（Write ahead log）方式产生的日志信息，内部是一个简单的顺序日志。每个RegionServer对应1个Hlog(备注：1.x版本的可以开启MultiWAL功能，允许多个Hlog)，所有对于该RegionServer的写入都被记录到Hlog中。Hlog实现的功能就是我们前面讲到的保证数据安全。当RegionServer出现问题的时候，能跟进Hlog来做数据恢复。此外为了保证恢复的效率，Hbase会限制最大保存的Hlog数量，如果达到Hlog的最大个数（hase.regionserver.max.logs参数控制）的时候，就会触发强制刷盘操作。对于已经刷盘的数据，其对应的Hlog会有一个过期的概念，Hlog过期后，会被监控线程移动到.oldlogs，然后会被自动删除掉。
### Hlog结构
- 都个Region共享一个Hlog文件，
- 单个Region在Hlog中是按照时间顺序存储的，
- 但是多个Region可能并不是完全按照时间顺序
- 每个Hlog最小单元由Hlogkey和WALEdit两部分组成。Hlogky由sequenceid、timestamp、cluster ids、regionname以及tablename等组成，WALEdit是由一系列的KeyValue组成，对一行上所有列（即所有KeyValue）的更新操作，都包含在同一个WALEdit对象中，这主要是为了实现写入一行多个列时的原子性。
```
    Hlog
        Hlogkey
            sequenceid ： 一个store级别的自增序列号，region的数据恢复和Hlog过期清除都要依赖这个信息
            timestamp
            cluster ids
            regionname
            tablename
        WALEdit
            n个 KeyValue
```
- sequenceid的相关逻辑：Memstore在达到一定的条件会触发刷盘的操作，刷盘的时候会获取刷新到最新的一个sequenceid的下一个sequenceid，并将新的sequenceid赋给oldestUnflushedSequenceId，并刷到Ffile中。
- Hlog文件对应所有Region的store中最大的sequenceid如果已经刷盘，就认为Hlog文件已经过期，就会移动到.oldlogs，等待被移除
- 当RegionServer出现故障的时候，需要对Hlog进行回放来恢复数据。回放的时候会读取Hfile的oldestUnflushedSequenceId中的sequenceid和Hlog中的sequenceid进行比较，小于sequenceid的就直接忽略，但与或者等于的就进行重做。回放完成后，就完成了数据的恢复工作
### Hlog的生命周期
- 产生:所有涉及到数据的变更都会先写Hlog，除非是你关闭了Hlog
- 滚动:Hlog的大小通过参数hbase.regionserver.logroll.period控制，默认是1个小时，时间达到hbase.regionserver.logroll.period 设置的时间，Hbase会创建一个新的Hlog文件。这就实现了Hlog滚动的目的。Hbase通过hbase.regionserver.maxlogs参数控制Hlog的个数。滚动的目的，为了控制单个Hlog文件过大的情况，方便后续的过期和删除。
- 过期:Hlog的过期判断依赖于sequenceid。Hbase会将Hlog的sequenceid和Hfile最大的sequenceid（刷新到的最新位置）进行比较，如果该Hlog文件中的sequenceid比刷新的最新位置的sequenceid都要小，那么这个Hlog就过期了，过期了以后，对应Hlog会被移动到.oldlogs目录。
- 要将过期的Hlog移动到.oldlogs目录，而不是直接删除，原因如下：
  - 因为Hbase还有一个主从同步的功能，这个依赖Hlog来同步Hbase的变更，有一种情况不能删除Hlog，那就是Hlog虽然过期，但是对应的Hlog并没有同步完成，因此比较好的做好是移动到别的目录。再增加对应的检查和保留时间。
删除
  - 如果Hbase开启了replication，当replication执行完一个Hlog的时候，会删除Zoopkeeper上的对应Hlog节点。在Hlog被移动到.oldlogs目录后，Hbase每隔hbase.master.cleaner.interval（默认60秒）时间会去检查.oldlogs目录下的所有Hlog，确认对应的Zookeeper的Hlog节点是否被删除，如果Zookeeper 上不存在对应的Hlog节点，那么就直接删除对应的Hlog。
  - hbase.master.logcleaner.ttl（默认10分钟）这个参数设置Hlog在.oldlogs目录保留的最长时间。
# 8.数据同样存在 HDFS，为什么 HBase 支持在线查询
- HBase 划分了多个 region，例如 1TB 分 500 个，那么最多只用读 2GB
- 列式存储：例如 region 分了 3 个列簇，那么 2GB / 3 = 666M，一个列簇有分为多个 HStoreFile，假如一个是 128M，总共有 6 个，一个在内存，剩下 5 个在磁盘
- 排序：记录排好序的，平均只需要遍历一半即 2.5 个 HStoreFile 共 300M
- kv 存储：只需要遍历 key 的位置就能判断
- 实时查询就是从内存中查询，HBase 数据先写入内存，达到一定的量再写入磁盘，在内存中只增加数据，用户的操作在内存中完成，保证实时响应。
# 9.Spark Streaming与 Flink与什么区别
- Flink 是标准的实时处理引擎，基于事件驱动。Spark Streaming 是微批（Micro-Batch）的模型。
- 时间机制：Spark Streaming 支持的时间机制有限，只支持处理时间。 Flink 支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时间。同时也支持 watermark 机制来处理滞后数据。
- 容错机制对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰好一次处理语义。Flink 则使用两阶段提交协议来解决这个问题。
# 10.有三个map，一个reduce来做top10，哪种方法最优。数据量特别大。
- 直接在 Reduce 中排序。
- 定义一个 size = 10 的小顶堆，大于堆顶元素才入堆。
- 利用 MapReduce 的高级 API 编程，定义分区器和分组比较器。
# 11.数据仓库的模型设计
- 概念模型CDM：概念模型是最终用户对数据存储的看法，反映了最终用户综合性的信息需求，以数据类的方式描述企业级的数据需求。概念模型的内容包括重要的实体与实体之间的关系，在概念模型中不包含实体的属性，也不包含定义实体的主键概念模型的目的是统一业务概念，作为业务人员和技术人员之间的沟通桥梁，确定不同实体之间的最高层次的关系
- 逻辑模型LDM：逻辑模型反映的是系统分析人员对数据存储的观点，是对概念模型的进一步分解和细化，逻辑模型是根据业务规则确定的，关于业务对象，业务对象的数据项以及业务对象之间关系的基本蓝图。逻辑模型的内容包括所有的实体和关系，确定每个实体的属性，定义每个实体的主键，指定实体的外键，需要进行范式化处理。逻辑模型的目标是尽可能详细的描述数据，并不考虑物理上如何实现
- 物理模型PDM：物理模型是在逻辑模型的基础上，考虑各种具体的技术实现因素，进行数据体系结构设计，真正实现数据在数据仓库中的存放。物理模型的内容包括确定所有的表和列，定义外键用确认表之间的关系，基于用户的需求可能要进行反范式化等内容。
# 12.数据仓库的数据清洗
#### 数据清洗的定义和意义
- 数据清洗是指通过一定的方法和工具，对原始数据进行清洗、过滤、校验和修复，以提高数据的一致性、准确性和可信度。数据清洗的意义在于解决数据仓库中存在的数据噪声、缺失、重复、异常等问题，从而提高数据质量和价值。在信息管理中，数据清洗不仅有助于减少数据分析误差，还可以提高决策的准确性和有效性。
### 数据清洗的分类

根据数据清洗的对象和目的，可以分为以下几类：
- 缺失值处理：针对数据缺失部分进行填充、删除或推断。可通过平均值、中位数、众数等方法进行填充，或通过插值法、回归法等进行推断。
- 重复数据处理：去除重复数据，保留唯一值。可通过哈希算法、余弦相似度等方法进行去重。
- 数据格式规范化：将不同格式的数据统一为规范格式。例如，将日期格式统一为ISO 8601标准。
- 数据类型转换：将某种数据类型转换为另一种数据类型。例如，将字符串类型的“123”转换为整数类型的123。
- 数据噪声处理：去除或平滑噪声数据，提高数据可靠性。可通过设置阈值、统计分析等方法进行处理。

以一个具体案例为例，假设某电商企业需要对用户购买行为数据进行清洗。针对以下问题，可以采用以下方法：
- 用户购买商品时填写的地址信息缺失或不一致。
  - 处理方法:对于缺失的地址信息，可以通过该用户的注册信息、历史订单信息等推断出可能地址；对于不一致的地址信息，可以通过地址规范化算法进行处理，如将“北京市朝阳区”统一为“北京市朝阳区”。
- 用户购买多个商品时，存在重复购买的情况。
  - 处理方法:可以通过去重算法，如余弦相似度算法，对重复购买的商品进行去重，保留唯一值。
- 用户购买商品时填写的电话号码格式不规范。
  - 处理方法:对于不规范的电话号码格式，可以通过正则表达式进行匹配和转换，如将“139XXXXXXX”转换为“xxxxxxxx”。
- 用户购买商品的价格存在异常值，可能为噪声数据。
  - 处理方法:可以通过盒须图等方法，检测并去除异常值，保持数据的正常分布。
### 数据清洗的技术
在数据清洗过程中，常采用以下技术:
- 机器学习：通过机器学习算法对数据进行分类、聚类等操作，识别并处理异常值、缺失值等。例如，使用决策树、随机森林等算法进行特征选择和去重。
- 深度学习：利用深度神经网络等深度学习模型，对数据进行清洗和预处理。例如，使用自编码器、卷积神经网络等进行特征提取和降维处理。
- 传统统计学方法：通过传统统计学方法，如滤波法、插值法等对数据进行清洗和处理。例如，使用移动平均滤波法对时间序列数据进行平滑处理。

各种技术在不同场景下具有不同的优缺点。例如，机器学习算法在处理复杂特征时具有优势；而传统统计学方法在处理时间序列数据时更为高效。在实际应用中，应根据具体需求选择合适的技术和方法。

### 数据清洗的流程

数据清洗通常包括以下流程：
1. 需求分析：明确数据清洗的目标和需求，确定需要清洗的数据范围和类型。
2. 数据采集：收集并获取需要清洗的数据，确保数据的完整性和准确性。
3. 数据预处理：对原始数据进行预处理，包括数据格式转换、数据类型转换等。
4. 数据去噪：对数据进行去噪处理，去除噪声数据和异常值。
5. 特征提取：根据清洗需求，提取数据的特征并进行处理。例如，通过特征选择和特征降维等方法处理高维数据。
6. 数据转换：将数据进行必要的转换和映射，如将日期格式转换为标准格式。
7. 数据验证：对清洗后的数据进行验证，确保数据的准确性和完整性。
8. 数据存储：将清洗后的数据存储到合适的数据存储系统中，以便后续分析和应用。
# 13.数据仓库是怎么设计的
开发数据仓库的过程包括以下几个步骤。
### 1.系统分析，确定主题
建立数据仓库的第一个步骤就是通过与业务部门的充分交流，了解建立数据仓库所要解决的问题的真正含义。确定各个主题下的查询分析要求。业务人员往往会罗列出很多想解决的问题，信息部门的人员应该对这些问题进行分类汇总，确定数据仓库所实现的业务功能。一旦确定问题以后，信息部门的人员还需要确定一下几个因素:
- 操作出现的频率，即业务部门每隔多长时间做一次查询分析。

- 在系统中需要保存多久的数据，是一年、两年还是五年、十年。

- 用户查询数据的主要方式，如在时间维度上是按照自然年，还是财政年。

- 用户所能接受的响应时间是多长、是几秒钟，还是几小时。

- 由于双方在理解上的差异，确定问题和了解问题可能是一个需要多次往复的过程，信息部门的人员可能需要做一些原型演示给业务部门的人员看，以最终确定系统将要实现的功能确实是业务部门所需要的。

### 2.选择满足数据仓库系统要求的软件平台
在数据仓库所要解决的问题确定后，第二个步骤就是选择合适的软件平台，包括数据库、建模工具、分析工具等。这里有许多因素要考虑，如系统对数据量、响应时间、分析功能的要求等，以下是一些公认的选择标准:
- 厂商的背景和支持能力，能否提供全方位的技术支持和咨询服务。
- 数据库对大数据量（TB级）的支持能力。
- 数据库是否支持并行操作。
- 能否提供数据仓库的建模工具，是否支持对元数据的管理。
- 能否提供支持大数据量的数据加载、转换、传输工具（ETT）。
- 能否提供完整的决策支持工具集，满足数据仓库中各类用户的需要。
### 3.建立数据仓库的逻辑模型
1. 确定建立数据仓库逻辑模型的基本方法。
2. 基于主题视图，把主题视图中的数据定义转到逻辑数据模型中。
3. 识别主题之间的关系。
4. 分解多对多的关系。
5. 用范式理论检验逻辑数据模型。
6. 由用户审核逻辑数据模型。
### 4.逻辑数据模型转化为数据仓库数据模型
- 删除非战略性数据：数据仓库模型中不需要包含逻辑数据模型中的全部数据项，某些用于操作处理的数据项要删除。
- 增加时间主键：数据仓库中的数据一定是时间的快照，因此必须增加时间主键。
- 增加派生数据：对于用户经常需要分析的数据，或者为了提高性能，可以增加派生数据。
- 加入不同级别粒度的汇总数据：数据粒度代表数据细化程度，粒度越大，数据的汇总程度越高。粒度是数据仓库设计的一个重要因素，它直接影响到驻留在数据仓库中的数据量和可以执行的查询类型。显然，粒度级别越低，则支持的查询越多；反之，能支持的查询就有限。
- 对数据操作的效率与能得到数据的详细程度是一对矛盾，通常，人们希望建成的系统既有较高的效率，又能得到所需的详细资料。实施数据仓库的一个重要原则就是不要试图包括所有详细数据，因为90%的分析需求是在汇总数据上进行的。试图将粒度细化到最低层，只会增加系统的开销，降低系统的性能。

### 5.数据仓库数据模型优化
数据仓库设计时，性能是一项主要考虑因素。在数据仓库建成后，也需要经常对其性能进行监控，并随着需求和数据量的变更进行调整。优化数据仓库设计的主要方法是:
- 合并不同的数据表。
- 通过增加汇总表避免数据的动态汇总。
- 通过冗余字段减少表连接的数量，不要超过3~5个。
- 用ID代码而不是描述信息作为键值。
- 对数据表做分区。
### 6.数据清洗转换和传输
由于业务系统所使用的软硬件平台不同，编码方法不同，业务系统中的数据在加载到数据仓库之前，必须进行数据的清洗和转换，保证数据仓库中数据的一致性。在设计数据仓库的数据加载方案时，必须考虑以下几项要求：
- 加载方案必须能够支持访问不同的数据库和文件系统。
- 数据的清洗、转换和传输必须满足时间要求，能够在规定的时间范围内完成。
- 支持各种转换方法，各种转换方法可以构成一个工作流。
- 支持增量加载，只把自上一次加载以来变化的数据加载到数据仓库。

### 7.开发数据仓库的分析应用

建立数据仓库的最终目的是为业务部门提供决策支持能力，必须为业务部门选择合适的工具实现其对数据仓库中的数据进行分析的要求。信息部门所选择的开发工具必须能够:
- 满足用户的全部分析功能要求。数据仓库中的用户包括了企业中各个业务部门，他们的业务不同，要求的分析功能也不同。如有的用户只是简单的分析报表，有些用户则要求做预测和趋势分析。
- 提供灵活的表现方式。分析的结果必须能够以直观、灵活的方式表现，支持复杂的图表。使用方式上，可以是客户机/服务器方式，也可以是浏览器方式。

事实上，没有一种工具能够满足数据仓库的全部分析功能需求，一个完整的数据仓库系统的功能可能是由多种工具来实现，因此必须考虑多个工具之间的接口和集成性问题，对于用户来说，希望看到的是一致的界面。

### 8.数据仓库的管理
只重视数据仓库的建立，而忽视数据仓库的管理必然导致数据仓库项目的失败。数据仓库管理主要包括数据库管理和元数据管理。数据库管理需要考以下几个方面:
- 安全性管理。数据仓库中的用户只能访问到他的授权范围内的数据，数据在传输过程中的加密策略。
- 数据仓库的备份和恢复。数据仓库的大小和备份的频率直接影响到备份策略。
- 如何保证数据仓库系统的可用性，硬件还是软件方法。
- 数据老化。设计数据仓库中数据的存放时间周期和对过期数据的老化方法，如历史数据只保存汇总数据，当年数据保存详细记录。

然而，元数据管理贯穿于整个系统的建设过程中，元数据是描述数据的数据。在数据采集阶段，元数据主要包括下列信息：
- 源数据的描述定义：类型、位置、结构。
- 数据转换规则：编码规则、行业标准。
- 目标数据仓库的模型描述：星型/雪花模型定义，维/事实结构定义。
- 源数据到目标数据仓库的映射关系：函数/表达式定义。
- 代码：生成转换程序、自动加载程序等。

在数据管理阶段，元数据主要包括下列信息：
- 汇总数据的描述：汇总/聚合层次、物化视图结构定义。
- 历史数据存储规则：位置、存储粒度。
- 多维数据结构描述：立方体定义、维结构、度量值、钻取层次定义等。

在数据展现阶段，元数据主要包括以下信息：

- 报表的描述：报表结构的定义。
- 统计函数的描述：各类统计分析函数的定义。
- 结果输出的描述：图、表输出的定义。

元数据不但是独立存放，而且对用户是透明的，标准元数据之间可以互相转换。

# 14.数仓规范设计哪些方面
- 字段、维度，存储压缩、数据保留机制
# 15.数仓质量怎么监控
- 数据质量管理系统，主键唯一、非空、数据波动
# 16.数仓主题分哪些
- 按照公司业务归类：申请单、客户信息、合同信息、放款、还款、余额、逾期等
# 17.数仓拉链表的原理
1. 先把变化表数据拿出来
2. union all (原表数据 left join 变化表,left join 右表如果有数据,那么就更新这条数据的end_date,没有就保持原数据不变)
3. 把变化数据 union all (原数据 left join 变化数据) 覆盖到原表
4. 下一次就又拿这个表来 left join 和 union all 下一次变化的数据
# 18. 数据倾斜的问题（场景、解决方式）
- Spark中的数据倾斜问题主要指shuffle过程中出现的数据倾斜问题，是由于不同的key对应的数据量不同导致的不同task所处理的数据量不同的问题。其主要有两个表现:
  - Spark作业的大部分task都执行迅速，只有有限的几个task执行的非常慢，此时可能出现了数据倾斜，作业可以运行，但是运行得非常慢
  - Spark作业的大部分task都执行迅速，但是有的task在运行过程中会突然报出OOM，反复执行几次都在某一个task报出OOM错误，此时可能出现了数据倾斜，作业无法正常运行

解决方案:
- 避免不必要的shuffle：如使用广播小表的方式，将reduce-side-join提升为map-side-join。
- 过滤导致倾斜的key ：如果在Spark作业中允许丢弃某些数据，那么可以考虑将可能导致数据倾斜的key进行过滤，滤除可能导致数据倾斜的key对应的数据。
- sample采样对倾斜key单独进行join：当数据量非常大时，可以考虑使用sample采样获取10%的数据，然后分析这10%的数据中哪个key可能会 导致数据倾斜，然后将这个key对应的数据单独提取出来 。
- 提高shuffle操作中的reduce并行度：提高reduce端并行度并没有从根本上改变数据倾斜的本质和问题，只是尽可能地去缓解和减轻shuffle reduce task的数据压力以及数据倾斜的问题。
# 19.数仓重点调度任务的保障方式
- 调整调度系统优先级
# 20.数仓任务报错和监控
- 调度系统捕捉错误，电话短信告之值班人员
# 21.MySQL引擎是什么，常用的是哪个，知道原理吗
- InnoDB 引擎、MyISAM 引擎、Memory 引擎
- 常用的是InnoDB
- InnoDB是一个将表中的数据存储到磁盘上的存储引擎，所以即使关机后重启我们的数据还是存在的。而真正处理数据的过程是发生在内存中的，所以需要把磁盘中的数据加载到内存中，如果是处理写入或修改请求的话，还需要把内存中的内容刷新到磁盘上。而我们知道读写磁盘的速度非常慢，和内存读写差了几个数量级，所以当我们想从表中获取某些记录时，存储引擎需要把数据从磁盘上读取出来。
# 22.MySQL怎么建索引？
- 选择合适的字段属性创建索引即可。
# 23.Sqoop增量导入数据怎么实现
```
sqoop import \
--connect jdbc:mysql://192.168.88.80:3306/userdb \
--username root \
--password 123456 \
--target-dir /sqoop/result5 \
--query 'select id,name,deg from emp WHERE  id>1203 and $CONDITIONS' \
--delete-target-dir \
--fields-terminated-by '\t' \
--m 1
```
# 24.Flume事务实现
# 25.Kafka消费者角度考虑是拉取数据还是推送数据
# 26.Kafka中的数据是有序的吗
# 27.Kafka数据推送失败怎么处理
# 28.Kafka保证生产者精准一次
# 29.没有接受到ack才会出现声明情况
# 30.Kafka数据重复怎么处理
# 31.Spark Streaming怎么里面实现精准一次消费
# 32.如果offset没有发送成功数据会怎样
# 33.Hive的优化，项目中怎么优化的(我说了join的一些优化)
# 34.然后就问了用MR怎么实现join,手写代码
# 35.数据倾斜怎么处理的，
# 36.碰到过oom情况吗，什么原因导致的，怎么处理的
# 37.Hbase有那些组件，
# 38.什么场景会用到Hbase
# 39.Hbase的读写流程，大概说一下
# 40.Spark,任务提交的流程，
# 41.Spark的两种核心Shuffle，未优化的和优化的
# 42.常用的数据结构都有哪些
# 43.怎么实现一个list,怎么实现一个mapmap一般什么场景使用
# 44.用过树这种结构吗，什么场景用到的
# 45.用Cannal监控mysql之后，Scala怎么通过SparkStreaming去消费的
# 46.你们这个项目spark计算完之后存HBase怎么设计的
# 47.HBase你们在项目中怎么设计rowkey的
# 48.给2个有序数组，合并成一个有序数组延伸：给n个有序数组，合并成一个有序数组
# 49.sql统计观看时间在0-1分钟的用户数，1-10分钟的用户数
# 50.选型为什么是spark而不是Flink，有哪些优缺点（我项目上写的spark）
# 51.HBASE为什么快（列式存储，内存，lsm树）
# 52.消费不到kafka数据怎么办
# 53.kafka怎么监控，重分区
# 54.g1回收器和cms区别
# 55.jvm调整过没有
# 56.数据仓库的基础理论:建模,分层
# 57.flink容错机制
# 58.spark调优，数据倾斜的处理方式
# 60.算法:聚类kmeans
# 61.二叉树算法
# 62.Java 线程问题，包括线程安全，锁，和线程池。
# 63.Hbasse和mysql区别和存储原理
# 64.kafka的底层原理，kafka如何保证全局消费数据有序
# 65.redis底层原理，如何预估数据量
# 66.yarn底层原理，执行流程和，yarn底层使用的算法
# 67.算法基础建模有哪些，如何进行数据挖掘
# 68.链表反转排序
# 69.sql 连续日期活跃用户
# 70.hive udf udaf udtf spark
# 71.任务提交过程
# 72.回文函数
# 73.hive 数据倾斜
# 74.redis 跳表
# 75.spring ioc
# 76.spring 需要单例设置的参数
# 77.线程池几个配置参数含义
# 78.mysql 事务 a b 客户端提交事务处理
# 79.storm 实时
# 80.java 并发包
# 81.hive 如何转mr
# 82.线程 可重入理解
# 83.算法题 一个LRU  一个深度优先搜索
# 84.flink 的watermark  shardGroup的概念
# 85.kafka如何保证消息的有序行高可用  
# 86.数据倾斜如何处理 
# 87.flink watermark 和barries 和checkpoint
# 88.flink和spark的区别
# 89.jvm常见的垃圾回收算法
# 90.heap怎么分带的
# 91.kafka调优
