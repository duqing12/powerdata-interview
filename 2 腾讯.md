# 1.怎么查看表结构，表创建语句？怎么查看表有哪些分区？怎么查看分区对应hdfs路径？怎么计算某个分区的数据量大小？怎么计算某个分区的文件总数？
- 查看表结构 DESCRIBE table_name;
- 查看表创建语句 SHOW CREATE TABLE table_name;
- 查看表的分区信息 SHOW PARTITIONS table_name;
- 查看分区对应的HDFS路径 SHOW PARTITIONS table_name;
- 计算某个分区的数据量大小 SELECT SUM(filesize) FROM table_name WHERE partition_column = 'partition_value';
- 计算某个分区的文件总数 SELECT COUNT(*) FROM table_name WHERE partition_column = 'partition_value';
# 2.有一hive sql，怎么计算这个sql会产生多少个map数？
- 在Hive中执行EXPLAIN语句后，会输出SQL语句的执行计划，其中包含了Map任务的数量。你可以查找类似于Map Operator Tree:的部分，然后查看Map Operator Tree下的Statistics:行，其中的Num rows:就是Map任务的数量。

# 4.怎么查看hive有什么自带函数？怎么查看函数的详细信息？
- 查看Hive自带的函数列表：SHOW FUNCTIONS;
- 查看函数的详细信息：DESCRIBE FUNCTION <function_name>;
# 5.Hive支持哪些基木数据类型？
- TINYINT: 8位有符号整数
- SMALLINT: 16位有符号整数
- INT: 32位有符号整数
- BIGINT: 64位有符号整数
- BOOLEAN: 布尔类型，true或false
- FLOAT: 单精度浮点数
- DOUBLE: 双精度浮点数
- STRING: 字符串类型
- TIMESTAMP: 时间戳类型
- DATE: 日期类型
- BINARY: 二进制类型
# 6.请指出下面两个语句读取分区ds数据的不同

```
Select * from t where ds=20150101 and province='gd' or province='ex'
Select x fromt where ds 20150101 and（province='gd' or province='ex'）
```
这两个语句的主要区别是第一个语句选择了所有列（使用*表示），而第二个语句只选择了x列。另外，第一个语句的条件判断逻辑与第二个语句的条件判断逻辑也有所不同。

# 7.将字符串"keyl=valuel&key2=value2...keyn=valuen"进行分割放到一个字段里面，可以查出任意一个keyx对应的valuex值。请写出查询语句，并计算总共有多少个key值。

```
SELECT SUBSTRING_INDEX(SUBSTRING_INDEX(your_column, '&', n), '=', -1) AS value
FROM your_table
WHERE your_column LIKE '%keyx=%'

```
将 your_column 替换为包含字符串的字段名，将 your_table 替换为表名，将 keyx 替换为要查询的键名。
```
SELECT COUNT(DISTINCT SUBSTRING_INDEX(SUBSTRING_INDEX(your_column, '&', n), '=', 1)) AS key_count
FROM your_table

```



# 8.a表和b表内连接，a表为小表，只有2000行记录

```
select a.*from a Join b on a.key=b.key
```

可以进行怎样的优化

- 确保在 a 表和 b 表上都有适当的索引。在 a 表的 key 字段和 b 表的 key 字段上创建索引，可以加快连接操作的速度。

- 如果 a 表和 b 表的数据量都很大，可以考虑使用内存表（Memory Table）来替代临时表（Temporary Table）。内存表的查询速度更快，可以提高性能。

- 如果 a 表和 b 表的数据都不经常变动，可以考虑使用表格分区（Table Partitioning）来进行优化。将 a 表和 b 表分成多个分区，可以减少查询的数据量，提高查询速度。

- 如果 a 表和 b 表的数据量都很大，可以考虑使用并行查询（Parallel Query）来进行优化。将查询操作分成多个并行的任务，可以利用多核处理器的并行计算能力，加快查询速度。

- 可以通过调整数据库的配置参数来优化查询性能。例如，增加内存缓冲区的大小、调整连接池的大小等。


# 9.多表连按的写法：a,.b,c三个表内连接，连接字段都是key，怎样写连接语句？

```
SELECT *
FROM a
INNER JOIN b ON a.key = b.key
INNER JOIN c ON a.key = c.key;

```


# 10.两大表连接，发生了数据倾斜，有几个reduce无法完成，怎么查找发生数据领斜的原因？应该怎样优化？
- 查看两个表中的键值分布情况，可以使用Hive的ANALYZE TABLE命令来获取每个键值的数量和分布情况。
- 检查两个表的数据分布情况，例如使用Hive的DESCRIBE EXTENDED命令来查看表的统计信息，特别是numFiles和totalSize字段，以确定数据是否均匀分布。
- 检查两个表的数据倾斜情况，可以使用Hive的GROUP BY语句按照键值进行聚合，并查看每个键值的数量，以确定是否存在数据倾斜的情况。
对于优化数据倾斜的问题，可以尝试以下方法：
- 使用随机前缀或者哈希函数对连接键进行处理，以减少某些键值的数据量。例如，可以对连接键的值进行哈希运算，然后将哈希值的前缀作为新的连接键。
- 将数据量大的键值进行拆分，将其分散到多个reduce任务中处理。可以根据键值的分布情况，将数据量大的键值拆分成多个子键值，然后使用子键值进行连接操作。
- 考虑使用MapReduce的Combiner功能，在Map端进行一些预聚合操作，减少数据传输量。
- 调整reduce任务的数量，可以根据数据倾斜的情况，将数据量大的键值所在的reduce任务数量增加，以减少单个reduce任务的数据量。
- 使用其他分布式计算框架，如Spark，可以使用Spark的repartition或者broadcast操作来优化数据倾斜的问题。

# 15.如何用hivesql实现sqL中的exist/in 子句

```
SEL ECT a* FROM a where a.key in（select dstinct key from b where key like ‘filter%’）
```

# 18.reduceByKey()、groupByKey()有什么区别？
reduceByKey()和groupByKey()是Spark中的两个常用的转换操作。
- reduceByKey()操作是基于键值对的RDD，它按照键对值进行聚合操作。它将具有相同键的值聚合在一起，并对这些值应用一个指定的聚合函数，生成一个新的键值对RDD。reduceByKey()操作是一个宽依赖操作，即会将数据进行shuffle操作，将具有相同键的数据发送到同一个节点上进行聚合。
- groupByKey()操作也是基于键值对的RDD，它将具有相同键的值聚合在一起，但不会对值进行聚合操作。它将具有相同键的所有值放在一个迭代器中，并生成一个新的键值对RDD。groupByKey()操作是一个窄依赖操作，即不会进行shuffle操作，只是将具有相同键的数据放在一起。

因此，reduceByKey()操作更适合在数据量较大时进行聚合操作，而groupByKey()操作更适合在数据量较小时进行分组操作。在数据量较大时，reduceByKey()操作可以减少数据的传输量和计算量，提高效率。而在数据量较小时，groupByKey()操作可以更简单地将具有相同键的数据放在一起。
# 19.DataFrame和RDD有什么区别？
- 数据类型：RDD是一个弹性分布式数据集，它是一个由分区组成的元素集合，每个分区都可以在集群中的不同节点上进行计算。RDD中的元素可以是任意类型的对象。而DataFrame是一个以列为基础的分布式数据集，它具有类似于关系型数据库中表的结构，每个列都有一个名称和数据类型。
- 数据处理：RDD提供了一组丰富的转换操作（如map、filter、reduce等），可以对数据进行自定义的处理。而DataFrame提供了更高级的API，可以进行更高效的数据操作，如过滤、排序、聚合等。
- 优化执行：DataFrame在底层使用了Catalyst优化器，可以对查询进行优化和重写，以提高执行效率。而RDD没有这种优化器，需要手动编写更底层的代码来实现性能优化。
- 数据源：DataFrame可以直接从多种数据源中读取数据，如Hive、HBase、JSON、CSV等，而RDD需要手动编写代码来解析和处理数据。


# 24.在Hive执行语句的时候如果很慢，什么原因造成
- 数据量过大：如果源表和目标表都包含大量的数据，那么执行INSERT INTOTABLE...SELECTQ语句可能会非常耗时。这是因为Hive默认会将所有的数据都加载到内存中进行处理，而大量的数据可能会导致内存溢出，从而降低查询的性能。在这种情况下，可以考虑增加集群的资源，如增加节点的数量或者调整集群的配置，以提高处理大数据量的能力。
- 数据倾斜：如果源表中的数据存在倾斜现象，即某些数据分布不均匀，这可能导致INSERT INTO TABLE...SELECT语句执行时的性能问题。在这种情况下，可以考虑使用Hive的一些优化技术，如数据分桶(bucketing)或者数据分区(partitioning)，将数据按照某个字段进行分割存储，从而提高查询的效率。

# 27.维度建模有什么好处？ 为什么选择这个？
- 数据冗余小（因为很多具体的信息都存在相应的维度表中了，比如客户信息就只有一份）
- 结构清晰（表结构一目了然）
- 便于做OLAP分析（数据分析用起来会很方便）
- 增加使用成本，比如查询时要关联多张表
- 数据不一致，比如用户发起购买行为的时候的数据，和我们维度表里面存放的数据不一致


# 31.你知道Spark的宽窄依赖吗？ 
- 窄依赖：父RDD的一个分区的数据给子RDD的一个分区(不需要调用Shuffle的分区器），一个Stage内部的计算都是窄依赖的过程，全部在内存中完成。
- 宽依赖：父RDD的一个分区数据给子RDD的多个分区（需要调用Shuffle的分区器来实现），Spark的job中按照宽依赖来划分Stage。

设计宽窄依赖的好处
- 提高数据容错的性能，避免分区数据丢失时，需要重新构建整个RDD
  - 场景：如果子RDD的某个分区的数据丢失
        
      无标记：不清楚父RDD与子RDD数据之间的关系，必须重新构建整个父RDD所有数据
        
      有标记：父RDD一个分区只对应子RDD的一个分区，按照对应关系恢复父RDD的对应分区即可

- 提高数据转换的性能，将连续窄依赖操作使用同一个Task都放在内存中直接转换
  - 场景：如果RDD需要多个map、flatMap、filter、reduceByKey、sortByKey等算子的转换操作
      
      无标记：每个转换不知道会不会经过Shuffle，都使用不同的Task来完成，每个Task的结果要保存到磁盘
      
      有标记：多个连续窄依赖算子放在一个Stage中，共用一套Task在内存中完成所有转换，性能更快