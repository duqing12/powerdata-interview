# 1.wordcount的实现过程
### Map 阶段：
1. 按行读取要统计的文本文件；
2. 按空格拆分每行的单词
3. 将单词转换成 kv 键值对，格式为（单词，1）
4. 将所有的 kv 键值对中的单词按照单词首字母分区，比如分两个区，那么分区1（a-p），分区2（q-z）
### Reduce 阶段：
1. 每个 ReduceTask 拉取上一阶段所有 MapTask 的输出，按照 key 汇总
2. 对相同的 key 的 value（value 都是 1） 求和
# 2.MapReduce 与 Spark 的区别
- MapReduce 是一个分布式运算程序的编程框架。核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并行运行在一个Hadoop集群上。
- Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。
- Spark 和 Hadoop 的根本差异是多个作业之间的数据通信问题 :
  - Spark多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘。
  - Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互。
  - Spark 的弹性分布式数据集（Resilient Distributed Datasets），提供了比 MapReduce 丰富的模型，可以快速在内存中对数据集进行多次迭代，MapReduce 的设计初衷并不是为了满足循环迭代式数据流处理。
  - Spark 是基于内存的，由于内存的限制，可能会由于内存资源不够导致Job执行失败，数据量大时 MapReduce 是一个更好的选择。
# 3.Spark 在 client 与在 cluster 运行的区别
- Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试。
- Cluster模式将用于监控和调度的Driver模块启动在Yarn集群资源中执行。一般应用于实际生产环境。
# 4.相同的 SQL 在 Hive SQL 与 Spark SQL 的实现中，为什么 Spark 比 Hadoop 快
- Hive 的底层时 MapReduce，所以比 Spark 慢
# 5.UDF 是什么
- UDF（User-Defined Functions）是用户定义的 hive 函数。
- UDF：one to one，一进一出，如：upper、substr函数
- UDAF：many to one，多进一出，如：sum、min。
- UDTF：one to many ，一进多出。如：alteral view 与 explode
# 6.设计 HBase 表需要注意的点
- Hbase 中一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计 rowkey的主要目的 ，就是让数据均匀的分布于所有的 region 中，在一定程度上防止数据倾斜。常用方法：
  - 生成随机数、hash、散列值。比如：原本 rowKey 为1001的哈希后变成:dd01903921ea24941c26a48f2cec24e0bb0e8cc7
  - 字符串反转:20170524000001转成10000042507102，20170524000002转成20000042507102
  - 字符串拼接:20170524000001_a12e，20170524000001_93i7
# 7.HBase的 Hlog
### Hlog简介
- Hlog是Hbase实现WAL（Write ahead log）方式产生的日志信息，内部是一个简单的顺序日志。每个RegionServer对应1个Hlog(备注：1.x版本的可以开启MultiWAL功能，允许多个Hlog)，所有对于该RegionServer的写入都被记录到Hlog中。Hlog实现的功能就是我们前面讲到的保证数据安全。当RegionServer出现问题的时候，能跟进Hlog来做数据恢复。此外为了保证恢复的效率，Hbase会限制最大保存的Hlog数量，如果达到Hlog的最大个数（hase.regionserver.max.logs参数控制）的时候，就会触发强制刷盘操作。对于已经刷盘的数据，其对应的Hlog会有一个过期的概念，Hlog过期后，会被监控线程移动到.oldlogs，然后会被自动删除掉。
### Hlog结构
- 都个Region共享一个Hlog文件，
- 单个Region在Hlog中是按照时间顺序存储的，
- 但是多个Region可能并不是完全按照时间顺序
- 每个Hlog最小单元由Hlogkey和WALEdit两部分组成。Hlogky由sequenceid、timestamp、cluster ids、regionname以及tablename等组成，WALEdit是由一系列的KeyValue组成，对一行上所有列（即所有KeyValue）的更新操作，都包含在同一个WALEdit对象中，这主要是为了实现写入一行多个列时的原子性。
```
    Hlog
        Hlogkey
            sequenceid ： 一个store级别的自增序列号，region的数据恢复和Hlog过期清除都要依赖这个信息
            timestamp
            cluster ids
            regionname
            tablename
        WALEdit
            n个 KeyValue
```
- sequenceid的相关逻辑：Memstore在达到一定的条件会触发刷盘的操作，刷盘的时候会获取刷新到最新的一个sequenceid的下一个sequenceid，并将新的sequenceid赋给oldestUnflushedSequenceId，并刷到Ffile中。
- Hlog文件对应所有Region的store中最大的sequenceid如果已经刷盘，就认为Hlog文件已经过期，就会移动到.oldlogs，等待被移除
- 当RegionServer出现故障的时候，需要对Hlog进行回放来恢复数据。回放的时候会读取Hfile的oldestUnflushedSequenceId中的sequenceid和Hlog中的sequenceid进行比较，小于sequenceid的就直接忽略，但与或者等于的就进行重做。回放完成后，就完成了数据的恢复工作
### Hlog的生命周期
- 产生:所有涉及到数据的变更都会先写Hlog，除非是你关闭了Hlog
- 滚动:Hlog的大小通过参数hbase.regionserver.logroll.period控制，默认是1个小时，时间达到hbase.regionserver.logroll.period 设置的时间，Hbase会创建一个新的Hlog文件。这就实现了Hlog滚动的目的。Hbase通过hbase.regionserver.maxlogs参数控制Hlog的个数。滚动的目的，为了控制单个Hlog文件过大的情况，方便后续的过期和删除。
- 过期:Hlog的过期判断依赖于sequenceid。Hbase会将Hlog的sequenceid和Hfile最大的sequenceid（刷新到的最新位置）进行比较，如果该Hlog文件中的sequenceid比刷新的最新位置的sequenceid都要小，那么这个Hlog就过期了，过期了以后，对应Hlog会被移动到.oldlogs目录。
- 要将过期的Hlog移动到.oldlogs目录，而不是直接删除，原因如下：
  - 因为Hbase还有一个主从同步的功能，这个依赖Hlog来同步Hbase的变更，有一种情况不能删除Hlog，那就是Hlog虽然过期，但是对应的Hlog并没有同步完成，因此比较好的做好是移动到别的目录。再增加对应的检查和保留时间。
删除
  - 如果Hbase开启了replication，当replication执行完一个Hlog的时候，会删除Zoopkeeper上的对应Hlog节点。在Hlog被移动到.oldlogs目录后，Hbase每隔hbase.master.cleaner.interval（默认60秒）时间会去检查.oldlogs目录下的所有Hlog，确认对应的Zookeeper的Hlog节点是否被删除，如果Zookeeper 上不存在对应的Hlog节点，那么就直接删除对应的Hlog。
  - hbase.master.logcleaner.ttl（默认10分钟）这个参数设置Hlog在.oldlogs目录保留的最长时间。
# 8.数据同样存在 HDFS，为什么 HBase 支持在线查询
- HBase 划分了多个 region，例如 1TB 分 500 个，那么最多只用读 2GB
- 列式存储：例如 region 分了 3 个列簇，那么 2GB / 3 = 666M，一个列簇有分为多个 HStoreFile，假如一个是 128M，总共有 6 个，一个在内存，剩下 5 个在磁盘
- 排序：记录排好序的，平均只需要遍历一半即 2.5 个 HStoreFile 共 300M
- kv 存储：只需要遍历 key 的位置就能判断
- 实时查询就是从内存中查询，HBase 数据先写入内存，达到一定的量再写入磁盘，在内存中只增加数据，用户的操作在内存中完成，保证实时响应。
# 9.Spark Streaming与 Flink与什么区别
- Flink 是标准的实时处理引擎，基于事件驱动。Spark Streaming 是微批（Micro-Batch）的模型。
- 时间机制：Spark Streaming 支持的时间机制有限，只支持处理时间。 Flink 支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时间。同时也支持 watermark 机制来处理滞后数据。
- 容错机制对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰好一次处理语义。Flink 则使用两阶段提交协议来解决这个问题。
# 10.有三个map，一个reduce来做top10，哪种方法最优。数据量特别大。
- 直接在 Reduce 中排序。
- 定义一个 size = 10 的小顶堆，大于堆顶元素才入堆。
- 利用 MapReduce 的高级 API 编程，定义分区器和分组比较器。
# 11.数据仓库的模型设计
- 概念模型CDM：概念模型是最终用户对数据存储的看法，反映了最终用户综合性的信息需求，以数据类的方式描述企业级的数据需求。概念模型的内容包括重要的实体与实体之间的关系，在概念模型中不包含实体的属性，也不包含定义实体的主键概念模型的目的是统一业务概念，作为业务人员和技术人员之间的沟通桥梁，确定不同实体之间的最高层次的关系
- 逻辑模型LDM：逻辑模型反映的是系统分析人员对数据存储的观点，是对概念模型的进一步分解和细化，逻辑模型是根据业务规则确定的，关于业务对象，业务对象的数据项以及业务对象之间关系的基本蓝图。逻辑模型的内容包括所有的实体和关系，确定每个实体的属性，定义每个实体的主键，指定实体的外键，需要进行范式化处理。逻辑模型的目标是尽可能详细的描述数据，并不考虑物理上如何实现
- 物理模型PDM：物理模型是在逻辑模型的基础上，考虑各种具体的技术实现因素，进行数据体系结构设计，真正实现数据在数据仓库中的存放。物理模型的内容包括确定所有的表和列，定义外键用确认表之间的关系，基于用户的需求可能要进行反范式化等内容。
# 12.数据仓库的数据清洗
#### 数据清洗的定义和意义
- 数据清洗是指通过一定的方法和工具，对原始数据进行清洗、过滤、校验和修复，以提高数据的一致性、准确性和可信度。数据清洗的意义在于解决数据仓库中存在的数据噪声、缺失、重复、异常等问题，从而提高数据质量和价值。在信息管理中，数据清洗不仅有助于减少数据分析误差，还可以提高决策的准确性和有效性。
### 数据清洗的分类

根据数据清洗的对象和目的，可以分为以下几类：
- 缺失值处理：针对数据缺失部分进行填充、删除或推断。可通过平均值、中位数、众数等方法进行填充，或通过插值法、回归法等进行推断。
- 重复数据处理：去除重复数据，保留唯一值。可通过哈希算法、余弦相似度等方法进行去重。
- 数据格式规范化：将不同格式的数据统一为规范格式。例如，将日期格式统一为ISO 8601标准。
- 数据类型转换：将某种数据类型转换为另一种数据类型。例如，将字符串类型的“123”转换为整数类型的123。
- 数据噪声处理：去除或平滑噪声数据，提高数据可靠性。可通过设置阈值、统计分析等方法进行处理。

以一个具体案例为例，假设某电商企业需要对用户购买行为数据进行清洗。针对以下问题，可以采用以下方法：
- 用户购买商品时填写的地址信息缺失或不一致。
  - 处理方法:对于缺失的地址信息，可以通过该用户的注册信息、历史订单信息等推断出可能地址；对于不一致的地址信息，可以通过地址规范化算法进行处理，如将“北京市朝阳区”统一为“北京市朝阳区”。
- 用户购买多个商品时，存在重复购买的情况。
  - 处理方法:可以通过去重算法，如余弦相似度算法，对重复购买的商品进行去重，保留唯一值。
- 用户购买商品时填写的电话号码格式不规范。
  - 处理方法:对于不规范的电话号码格式，可以通过正则表达式进行匹配和转换，如将“139XXXXXXX”转换为“xxxxxxxx”。
- 用户购买商品的价格存在异常值，可能为噪声数据。
  - 处理方法:可以通过盒须图等方法，检测并去除异常值，保持数据的正常分布。
### 数据清洗的技术
在数据清洗过程中，常采用以下技术:
- 机器学习：通过机器学习算法对数据进行分类、聚类等操作，识别并处理异常值、缺失值等。例如，使用决策树、随机森林等算法进行特征选择和去重。
- 深度学习：利用深度神经网络等深度学习模型，对数据进行清洗和预处理。例如，使用自编码器、卷积神经网络等进行特征提取和降维处理。
- 传统统计学方法：通过传统统计学方法，如滤波法、插值法等对数据进行清洗和处理。例如，使用移动平均滤波法对时间序列数据进行平滑处理。

各种技术在不同场景下具有不同的优缺点。例如，机器学习算法在处理复杂特征时具有优势；而传统统计学方法在处理时间序列数据时更为高效。在实际应用中，应根据具体需求选择合适的技术和方法。

### 数据清洗的流程

数据清洗通常包括以下流程：
1. 需求分析：明确数据清洗的目标和需求，确定需要清洗的数据范围和类型。
2. 数据采集：收集并获取需要清洗的数据，确保数据的完整性和准确性。
3. 数据预处理：对原始数据进行预处理，包括数据格式转换、数据类型转换等。
4. 数据去噪：对数据进行去噪处理，去除噪声数据和异常值。
5. 特征提取：根据清洗需求，提取数据的特征并进行处理。例如，通过特征选择和特征降维等方法处理高维数据。
6. 数据转换：将数据进行必要的转换和映射，如将日期格式转换为标准格式。
7. 数据验证：对清洗后的数据进行验证，确保数据的准确性和完整性。
8. 数据存储：将清洗后的数据存储到合适的数据存储系统中，以便后续分析和应用。
# 13.数据仓库是怎么设计的
开发数据仓库的过程包括以下几个步骤。
### 1.系统分析，确定主题
建立数据仓库的第一个步骤就是通过与业务部门的充分交流，了解建立数据仓库所要解决的问题的真正含义。确定各个主题下的查询分析要求。业务人员往往会罗列出很多想解决的问题，信息部门的人员应该对这些问题进行分类汇总，确定数据仓库所实现的业务功能。一旦确定问题以后，信息部门的人员还需要确定一下几个因素:
- 操作出现的频率，即业务部门每隔多长时间做一次查询分析。

- 在系统中需要保存多久的数据，是一年、两年还是五年、十年。

- 用户查询数据的主要方式，如在时间维度上是按照自然年，还是财政年。

- 用户所能接受的响应时间是多长、是几秒钟，还是几小时。

- 由于双方在理解上的差异，确定问题和了解问题可能是一个需要多次往复的过程，信息部门的人员可能需要做一些原型演示给业务部门的人员看，以最终确定系统将要实现的功能确实是业务部门所需要的。

### 2.选择满足数据仓库系统要求的软件平台
在数据仓库所要解决的问题确定后，第二个步骤就是选择合适的软件平台，包括数据库、建模工具、分析工具等。这里有许多因素要考虑，如系统对数据量、响应时间、分析功能的要求等，以下是一些公认的选择标准:
- 厂商的背景和支持能力，能否提供全方位的技术支持和咨询服务。
- 数据库对大数据量（TB级）的支持能力。
- 数据库是否支持并行操作。
- 能否提供数据仓库的建模工具，是否支持对元数据的管理。
- 能否提供支持大数据量的数据加载、转换、传输工具（ETT）。
- 能否提供完整的决策支持工具集，满足数据仓库中各类用户的需要。
### 3.建立数据仓库的逻辑模型
1. 确定建立数据仓库逻辑模型的基本方法。
2. 基于主题视图，把主题视图中的数据定义转到逻辑数据模型中。
3. 识别主题之间的关系。
4. 分解多对多的关系。
5. 用范式理论检验逻辑数据模型。
6. 由用户审核逻辑数据模型。
### 4.逻辑数据模型转化为数据仓库数据模型
- 删除非战略性数据：数据仓库模型中不需要包含逻辑数据模型中的全部数据项，某些用于操作处理的数据项要删除。
- 增加时间主键：数据仓库中的数据一定是时间的快照，因此必须增加时间主键。
- 增加派生数据：对于用户经常需要分析的数据，或者为了提高性能，可以增加派生数据。
- 加入不同级别粒度的汇总数据：数据粒度代表数据细化程度，粒度越大，数据的汇总程度越高。粒度是数据仓库设计的一个重要因素，它直接影响到驻留在数据仓库中的数据量和可以执行的查询类型。显然，粒度级别越低，则支持的查询越多；反之，能支持的查询就有限。
- 对数据操作的效率与能得到数据的详细程度是一对矛盾，通常，人们希望建成的系统既有较高的效率，又能得到所需的详细资料。实施数据仓库的一个重要原则就是不要试图包括所有详细数据，因为90%的分析需求是在汇总数据上进行的。试图将粒度细化到最低层，只会增加系统的开销，降低系统的性能。

### 5.数据仓库数据模型优化
数据仓库设计时，性能是一项主要考虑因素。在数据仓库建成后，也需要经常对其性能进行监控，并随着需求和数据量的变更进行调整。优化数据仓库设计的主要方法是:
- 合并不同的数据表。
- 通过增加汇总表避免数据的动态汇总。
- 通过冗余字段减少表连接的数量，不要超过3~5个。
- 用ID代码而不是描述信息作为键值。
- 对数据表做分区。
### 6.数据清洗转换和传输
由于业务系统所使用的软硬件平台不同，编码方法不同，业务系统中的数据在加载到数据仓库之前，必须进行数据的清洗和转换，保证数据仓库中数据的一致性。在设计数据仓库的数据加载方案时，必须考虑以下几项要求：
- 加载方案必须能够支持访问不同的数据库和文件系统。
- 数据的清洗、转换和传输必须满足时间要求，能够在规定的时间范围内完成。
- 支持各种转换方法，各种转换方法可以构成一个工作流。
- 支持增量加载，只把自上一次加载以来变化的数据加载到数据仓库。

### 7.开发数据仓库的分析应用

建立数据仓库的最终目的是为业务部门提供决策支持能力，必须为业务部门选择合适的工具实现其对数据仓库中的数据进行分析的要求。信息部门所选择的开发工具必须能够:
- 满足用户的全部分析功能要求。数据仓库中的用户包括了企业中各个业务部门，他们的业务不同，要求的分析功能也不同。如有的用户只是简单的分析报表，有些用户则要求做预测和趋势分析。
- 提供灵活的表现方式。分析的结果必须能够以直观、灵活的方式表现，支持复杂的图表。使用方式上，可以是客户机/服务器方式，也可以是浏览器方式。

事实上，没有一种工具能够满足数据仓库的全部分析功能需求，一个完整的数据仓库系统的功能可能是由多种工具来实现，因此必须考虑多个工具之间的接口和集成性问题，对于用户来说，希望看到的是一致的界面。

### 8.数据仓库的管理
只重视数据仓库的建立，而忽视数据仓库的管理必然导致数据仓库项目的失败。数据仓库管理主要包括数据库管理和元数据管理。数据库管理需要考以下几个方面:
- 安全性管理。数据仓库中的用户只能访问到他的授权范围内的数据，数据在传输过程中的加密策略。
- 数据仓库的备份和恢复。数据仓库的大小和备份的频率直接影响到备份策略。
- 如何保证数据仓库系统的可用性，硬件还是软件方法。
- 数据老化。设计数据仓库中数据的存放时间周期和对过期数据的老化方法，如历史数据只保存汇总数据，当年数据保存详细记录。

然而，元数据管理贯穿于整个系统的建设过程中，元数据是描述数据的数据。在数据采集阶段，元数据主要包括下列信息：
- 源数据的描述定义：类型、位置、结构。
- 数据转换规则：编码规则、行业标准。
- 目标数据仓库的模型描述：星型/雪花模型定义，维/事实结构定义。
- 源数据到目标数据仓库的映射关系：函数/表达式定义。
- 代码：生成转换程序、自动加载程序等。

在数据管理阶段，元数据主要包括下列信息：
- 汇总数据的描述：汇总/聚合层次、物化视图结构定义。
- 历史数据存储规则：位置、存储粒度。
- 多维数据结构描述：立方体定义、维结构、度量值、钻取层次定义等。

在数据展现阶段，元数据主要包括以下信息：

- 报表的描述：报表结构的定义。
- 统计函数的描述：各类统计分析函数的定义。
- 结果输出的描述：图、表输出的定义。

元数据不但是独立存放，而且对用户是透明的，标准元数据之间可以互相转换。

# 14.数仓规范设计哪些方面
- 字段、维度，存储压缩、数据保留机制
# 15.数仓质量怎么监控
- 数据质量管理系统，主键唯一、非空、数据波动
# 16.数仓主题分哪些
- 按照公司业务归类：申请单、客户信息、合同信息、放款、还款、余额、逾期等
# 17.数仓拉链表的原理
1. 先把变化表数据拿出来
2. union all (原表数据 left join 变化表,left join 右表如果有数据,那么就更新这条数据的end_date,没有就保持原数据不变)
3. 把变化数据 union all (原数据 left join 变化数据) 覆盖到原表
4. 下一次就又拿这个表来 left join 和 union all 下一次变化的数据
# 18. 数据倾斜的问题（场景、解决方式）
- Spark中的数据倾斜问题主要指shuffle过程中出现的数据倾斜问题，是由于不同的key对应的数据量不同导致的不同task所处理的数据量不同的问题。其主要有两个表现:
  - Spark作业的大部分task都执行迅速，只有有限的几个task执行的非常慢，此时可能出现了数据倾斜，作业可以运行，但是运行得非常慢
  - Spark作业的大部分task都执行迅速，但是有的task在运行过程中会突然报出OOM，反复执行几次都在某一个task报出OOM错误，此时可能出现了数据倾斜，作业无法正常运行

解决方案:
- 避免不必要的shuffle：如使用广播小表的方式，将reduce-side-join提升为map-side-join。
- 过滤导致倾斜的key ：如果在Spark作业中允许丢弃某些数据，那么可以考虑将可能导致数据倾斜的key进行过滤，滤除可能导致数据倾斜的key对应的数据。
- sample采样对倾斜key单独进行join：当数据量非常大时，可以考虑使用sample采样获取10%的数据，然后分析这10%的数据中哪个key可能会 导致数据倾斜，然后将这个key对应的数据单独提取出来 。
- 提高shuffle操作中的reduce并行度：提高reduce端并行度并没有从根本上改变数据倾斜的本质和问题，只是尽可能地去缓解和减轻shuffle reduce task的数据压力以及数据倾斜的问题。
# 19.数仓重点调度任务的保障方式
- 调整调度系统优先级
# 20.数仓任务报错和监控
- 调度系统捕捉错误，电话短信告之值班人员
# 21.MySQL引擎是什么，常用的是哪个，知道原理吗
- InnoDB 引擎、MyISAM 引擎、Memory 引擎
- 常用的是InnoDB
- InnoDB是一个将表中的数据存储到磁盘上的存储引擎，所以即使关机后重启我们的数据还是存在的。而真正处理数据的过程是发生在内存中的，所以需要把磁盘中的数据加载到内存中，如果是处理写入或修改请求的话，还需要把内存中的内容刷新到磁盘上。而我们知道读写磁盘的速度非常慢，和内存读写差了几个数量级，所以当我们想从表中获取某些记录时，存储引擎需要把数据从磁盘上读取出来。
# 22.MySQL怎么建索引？
- 选择合适的字段属性创建索引即可。
# 23.Sqoop增量导入数据怎么实现
```
sqoop import \
--connect jdbc:mysql://192.168.88.80:3306/userdb \
--username root \
--password 123456 \
--target-dir /sqoop/result5 \
--query 'select id,name,deg from emp WHERE  id>1203 and $CONDITIONS' \
--delete-target-dir \
--fields-terminated-by '\t' \
--m 1
```
# 24.Flume事务实现
1. doPut(放入event)，该操作由source触发。
- 一般的source会在process方法中，将event提交到channel。但是spoolingdirectorysource比较特殊，它针对一个本地目录创建了ReliableSpoolingFileEventReader对象, 并起一个定时线程去读取其中的文件。在线程中，取到一批event后(文件中一行即为一个event)，直接调用ChannelProcessor的processEventBatch方法，将这批数据提交到channel中。
- 在ChannelProcessor对象中,调用栈为：reqChannel.put(event) -> BasicChannelSemantics.put -> BasicTransactionSemantics.put -> BasicTransactionSemantics.doPut。其中doPut是一个抽象方法，其具体实现放在各个channel的Transaction中。这里使用的memoryChannel。
2. getTransaction(获取事务)
- 该操作最终调用到绑定的channel对象，这里指MemoryChannel.java中的createTransaction方法，在该方法中创建了一个对应的MemoryTransaction对象，设置了channel的容量大小等属性。
3. doCommit
- 实现在channel对象的doCommit()方法中，当一批event正确的放到channel内存中后或者正确的从channel拿出后，调用该方法。
4. doTake(拿出event)
- 该操作由sink触发，最终调用到channel对象的doTake()方法中，依次从channel对象的queue中获取event，放到takeList中。
5. doRollback(回滚)
- sink写出失败的event，还保存channel对象的在takeList中。当调用这个方法时，说明没有进行前发生异常了，所以takeList并未clear。方法中会把takeList中的最后一个元素，循环取出放回queue的第一个,下次继续操作这些event。
# 25.Kafka消费者角度考虑是拉取数据还是推送数据
Kafka的消费者是采用pull（拉）模式来获取消息的。在Kafka中，消费者决定何时以自己的速度拉取消息，而不是生产者将消息推送给消费者。这种拉模式有一些好处：
- 消费者控制速率：在拉模式下，消费者可以根据自己的处理能力和需求来控制消息的获取速率。这意味着消费者不会被强制匹配生产者的速度，从而避免了潜在的资源浪费和性能问题。
- 灵活性：消费者可以自由地决定从Kafka主题中的哪个分区拉取消息，以及拉取多少消息。这种灵活性允许消费者根据其用例需求进行优化。
- 背压控制：在拉模式下，如果消费者处理消息的速度比生产者产生消息的速度慢，消费者可以选择减缓拉取速度，从而避免积压过多的未处理消息，这被称为背压（backpressure）控制。这有助于确保系统的稳定性。
- 重新处理能力：由于消费者可以控制消息的拉取，它们可以轻松地实现消息的重新处理，例如，处理失败的消息或重新处理特定时间段内的消息。
- 可伸缩性：拉模式允许您根据需要添加更多的消费者，以提高系统的吞吐量，而不需要更改生产者的行为。
# 26.Kafka中的数据是有序的吗
- Kafka无法保证消息的全局有序性，这是因为Kafka的设计中允许多个生产者并行地向同一个主题写入消息。而且，一个主题可能会被划分为多个分区，每个分区都可以在独立的生产者和消费者之间进行并行处理。因此，生产者将消息写入各自的分区，而这些分区可能会在不同的时间接收到消息，从而导致消息在整个主题中的顺序不确定。
# 27.Kafka数据推送失败怎么处理
- 同步发送模式（Sync Mode）：在同步发送模式下，生产者发送消息后会阻塞等待服务器的响应。如果发送失败，生产者会抛出异常（例如ProducerRecord发送异常）或返回错误信息。开发者可以捕获异常并根据需要进行重试、错误处理或日志记录。
- 异步发送模式（Async Mode）：在异步发送模式下，生产者发送消息后不会阻塞等待服务器的响应。相反，它会立即返回一个Future对象或通过回调函数处理发送结果。开发者可以在Future对象中获取发送结果，并根据需要进行重试、错误处理或日志记录。
- 重试机制：可以配置生产者实例的retries参数来启用自动重试机制。当发送失败时，生产者会自动进行重试，直到达到最大重试次数（通过retries参数设置）。重试机制可以帮助处理瞬时的网络故障或Kafka服务器不可用的情况。
- 错误处理（Error Handling）：生产者在发送消息时可能会遇到一些可恢复的错误（例如网络超时），或者一些不可恢复的错误（例如无效的主题或无法分配分区）。根据不同的错误类型，开发者可以采取不同的策略，如重试、按需忽略或终止发送。
# 28.Kafka保证生产者精准一次
- 启用幂等性，只需要将Producer的参数中enable.idompotence设置为true即可。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。
# 29.Kafka数据重复怎么处理
- 对每个生产者生成的每条数据，都添加由生产者id，分区号，随机的序列号组成的标识符： (producerid,partition,SequenceId),通过标识符对数据进行去重。
# 30.如果offset没有发送成功数据会怎样
当使用Kafka生产者发送消息时，每条消息都会分配一个唯一的偏移量（offset），用于标识消息在分区中的位置。如果发送消息时出现问题导致offset没有成功发送，可能会有以下情况发生：
- 消息丢失：如果offset没有成功发送，Kafka就不会认为消息已经成功写入分区，因此消息可能会丢失。
- 重复消息：如果offset没有成功发送，但实际上消息已经成功写入分区，当生产者重新发送相同的消息时，Kafka可能会将其视为重复消息，导致消息重复消费。
# 31.碰到过oom情况吗，什么原因导致的，怎么处理的
堆内存不足（Java heap space）
- 原因：代码中可能存在大对象分配，通常是一个大树组。可能存在内存泄露，导致多次GC之后，还是无法找到一块足够大的内存容纳当前对象，常见于使用了File等资源没有回收。超出预期的访问量、数据量，通常是上游系统请求流量飙升，常见于各类促销、秒杀活动，可以结合业务流量指标排查是否有尖状峰值。
- 解决方案：针对大部分的情况，通常只需要通过-Xmx参数调高JVM堆内存即可。如果仍然没有解决，可以参考下面几种情况进行处理。如果是超大对象，检查其合理性，比如是否一次查询了数据库全部结果，而没有做结果数限制。如果是业务峰值压力，可以考虑添加机器资源，或者做限流降级操作。如果是内存泄露，需要找到持有对象，修改代码设计，比如关闭没有释放的连接。
# 32.Hbase有那些组件
- Client
  - 整个Hbase集群的访问入口
  - Client当中包含了访问Hbase接口，此外，Client维护了对应了的cache加速Hbase访问，比如cache的.META.元数据的信息
  - 与HRegionServer进行数据读写类操作
  - 使用HBase RPC机制与HMaster和HRegionServer进行通信；
- Zookeeper
  - 高可用：通过Zookeeper来保证master的高可用，保证集群中只有一个master运行。如果正在服务的master产生异常，通过竞争机制，产生新的master提供服务。
  - 存储元数据的入口这一功能减轻了master节点的“负担，把元数据交给zookeeper进行管理， 存储HBase的schema和table元数据,Zookeeper是存储元数据的统一入口地址
- 监控RegionServer：通过Zoopkeeper来监控RegionServer的状态，当RegionSevrer有异常的时候，通过回调的形式通知Master RegionServer上下线的信息。此外集群的一些维护工作也交给Zookeeper
- HMaster
  - 为RegionServer分配Region
  - 维护整个集群的负载均衡，以及元数据信息
  - 处理Region/RegionServer的分配或者转移
  - 通过Zookeeper将自身位置发布给Client
- HRegionServer
  - 直接对接用户的读写请求，处理来自客户端的读写请求，是真正的“干活”的节点
  - 管理master为其分配的Region。
  - 负责存储HBase的实际数据
  - 负责和底层HDFS的交互，刷新缓存到HDFS以及存储数据到HDFS
  - 负责Region变大以后的拆分
  - 负责Storefile的合并工作
  - 维护Hlog
  - 执行压缩
- HRegion
  - Hbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。
- Hdfs
  - HDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用（Hlog存储在HDFS）的支持。
  - 提供元数据和表数据的底层分布式存储服务，数据多副本，保证的高可靠和高可用性
- Write-Ahead logs
  - HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。
  - 用户所有的数据写入put等都会经过regionserver的log然后再去写
  - Wal解决的什么问题：Hbaase ha的问题，hbase崩溃的时候，它可以恢复到崩溃前的状态.实现远程备份，客户端初始化一个对数据库改动的操作，比如，put，delete。修改操作被封装在一个key，value实例里面然后通过rpc调用发送给regionserver，regionserver先写log再去写hregion
- Hlog
  - Hlog是实现WAL的类，一个hregionserver对应一个Hlog的实例，一台服务器只有一个Hlog实例， 当region初始化的时候，Hlog会作为参数传递给hregion的构造函数，这样获取到Hlog的引用，可以实现打日志
  - Hlog最核心的是调用append方法，完成对于日志的追加写，出于对性能的考虑，put，delete，increment有一个开关函数set retowal函数传进布尔类型的参数，如果设置成false将禁用WAL，，
  - Hlog通过序列化的number追踪数据的改变，内部原子类型的log保证线程安全
- Store
  - HFile存储在Store中，一个Store对应HBase表中的一个列族。
- MemStore
  - 顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。
- HFile
  - 这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。StoreFile是以Hfile的形式存储在HDFS的。
# 33.什么场景会用到Hbase
- 对象存储:比如一些app的海量的图片、网页、新闻等对象，可以存储在HBase中，有些病毒公司的病毒库也可以存储在HBase中。
- 时空数据:主要是轨迹、气象网格之类，比如滴滴打车的轨迹数据主要存在HBase之中，另外大数据量的车联网企业，数据也都是存在HBase中。比如互联网出行，智慧物流与外卖递送，传感网与实时GIS等场景。
- 时序数据 ;时序数据就是分布在时间上的一系列数值。HBase之上有OpenTSDB模块，可以满足时序类场景的需求。比如我们有很多的设备、传感器，产生很多数据，如果规模不是特别大的厂家有几千个风机，每个风机有几百个指标，那么就会有一百万左右的时序数据，如果用采样每一秒会产生一百万个时间点，如果用传统数据库，那么每一秒会产生一百万次，持续地往MQ做一百万次，它会崩裂。并且查询也是个大问题，除了多维查询以外，我们还会额外地增加时间纬度，查看一段时间的数据。这时候HBase很好了满足了时序类场景的需求。
- 推荐画像:特别是用户的画像，是一个比较大的稀疏矩阵，蚂蚁的风控就是构建在HBase上。用户画像有用户数据量大，用户标签多，标签统计维度不确定等特点，适合HBase特性的发挥。
- 消息/订单:在电信领域、银行领域，不少的订单查询底层的存储，另外不少通信、消息同步的应用构建在HBase之上。
- Feed流:Feed流，是RSS中用来接收该信息来源更新的接口，简单的说就是持续更新并呈现给用户的内容。比如微信朋友圈中看到的好友的一条条状态，微博看到的你关注的人更新的内容，App收到的一篇篇新文章的推送，都算是feed流。
- NewSQL:HBase上有Phoenix的插件，可以满足二级索引、SQL的需求，对接传统数据需要SQL非事务的需求。从NoSQL到NewSQL，Phoenix或许是新的趋势。
# 34.Hbase的读写流程，大概说一下
Hbase读取数据的流程：
1. 是由客户端发起读取数据的请求，首先会与zookeeper建立连接
2. 从zookeeper中获取一个hbase:meta表位置信息，被哪一个regionserver所管理着.hbase:meta表：hbase的元数据表，在这个表中存储了自定义表相关的元数据，包括表名，表有哪些列簇，表有哪些reguion,每个region存储的位置，每个region被哪个regionserver所管理，这个表也是存储在某一个region上的，并且这个meta表只会被一个regionserver所管理。这个表的位置信息只有zookeeper知道。
3. 连接这个meta表对应的regionserver,从meta表中获取当前你要读取的这个表对应的regionsever是谁。当一个表多个region怎么办呢？如果我们获取数据是以get的方式，只会返回一个regionserver.如果我们获取数据是以scan的方式，会将所有的region对应的regionserver的地址全部返回。
4. 连接要读取表的对应的regionserver,从regionserver上的开始读取数据：
       读取顺序：memstore-->blockcache-->storefile-->Hfile中
       注意：如果是scan操作，就不仅仅去blockcache了，而是所有都会去找。

Hbase的写入数据流程：
1. 由客户端发起写数据请求，首先会与zookeeper建立连接
2. 从zookeeper中获取hbase:meta表被哪一个regionserver所管理
3. 连接hbase:meta表中获取对应的regionserver地址 (从meta表中获取当前要写入数据的表对应的region所管理的regionserver) 只会返回一个regionserver地址
4. 与要写入数据的regionserver建立连接，然后开始写入数据，将数据首先会写入到HLog，然后将数据写入到对应store模块中的memstore中.（可能会写多个），当这两个地方都写入完成之后，表示数据写入完成。
# 35.Spark,任务提交的流程
1. Client提交任务后，Client会启动一个Driver进程。
2. Client为当前的application向Master申请资源
3. Master找到满足资源的Worker节点，启动Executor（内有ThreadPool，可以跑Task）
4. Executor启动后，会反向注册给Driver（告诉Driver：你可以给我发Task，我有能力run）
5. Driver将task发送给Executor、监控Task的执行情况，并回收结果
# 36.g1回收器和cms区别
- 内存占用不同：CMS垃圾回收器在执行垃圾回收时，会暂停应用程序的执行，因此它的内存占用相对较低；而G1垃圾回收器则不会暂停应用程序的执行，但它需要更多的内存来存储堆空间中的对象。
- 垃圾收集方式不同：CMS垃圾回收器使用标记清除算法来收集垃圾，这种算法可以快速地清理老年代中的无用对象；而G1垃圾回收器使用分代算法，将堆空间分为多个区域，分别进行垃圾收集，并根据区域中的对象分布情况来进行垃圾收集。
- 垃圾回收时间不同：由于G1垃圾回收器使用分代算法和增量整理策略，因此它的垃圾回收时间相对较长；而CMS垃圾回收器则可以在短时间内完成垃圾回收。
- 并发性不同：CMS垃圾回收器在进行垃圾回收时会暂停应用程序的执行，因此它的并发性较差；而G1垃圾回收器则不会暂停应用程序的执行，因此它的并发性较好.
# 37.flink容错机制
- Checkpointing（检查点）：Flink 使用检查点机制来实现容错。检查点是任务状态的一致性快照，它包含了所有正在进行中的任务的状态信息。Flink 定期创建检查点，并将其保存到可靠的存储系统中。
- Exactly-Once（仅一次）语义：Flink 的容错机制可以确保“仅一次”语义，即在发生故障时，任务恢复后不会重放已经处理过的数据。这通过将检查点和流的操作顺序进行协调来实现。
- Savepoints（保存点）：保存点是一种手动创建的检查点，用于将应用程序的状态保存到外部存储系统中。保存点可以用于应用程序升级、迁移或测试等场景。
- 容错机制配置：Flink 提供了一些配置选项来调整容错机制的行为。例如，可以设置检查点的频率、保留的检查点数量以及故障恢复策略等。
- 容错处理：当发生故障时，Flink 会根据容错机制的配置进行相应的处理。它会从最近的检查点恢复状态，并重新执行失败的任务。
# 38.线程池几个配置参数含义
- corePoolSize：核心线程数
  - 核心线程会一直存活，及时没有任务需要执行
  - 当线程数小于核心线程数时，即使有线程空闲，线程池也会优先创建新线程处理
  - 设置allowCoreThreadTimeout=true（默认false）时，核心线程会超时关闭
- maxPoolSize：最大线程数
  - 当线程数>=corePoolSize，且任务队列已满时。线程池会创建新线程来处理任务
  - 当线程数=maxPoolSize，且任务队列已满时，线程池会拒绝处理任务而抛出异常
- keepAliveTime：线程空闲时间
  - 当线程空闲时间达到keepAliveTime时，线程会退出，直到线程数量=corePoolSize
  - 如果allowCoreThreadTimeout=true，则会直到线程数量=0
- allowCoreThreadTimeout：允许核心线程超时
- queueCapacity：任务队列容量（阻塞队列）
  - 当核心线程数达到最大时，新任务会放在队列中排队等待执行
- rejectedExecutionHandler：任务拒绝处理器
  - 当线程数已经达到maxPoolSize，切队列已满，会拒绝新任务
  - 当线程池被调用shutdown()后，会等待线程池里的任务执行完毕，再shutdown。如果在调用shutdown()和线程池真正shutdown之间提交任务，会拒绝新任务
- ThreadPoolExecutor执行顺序
线程池按以下行为执行任务
  - 当线程数小于核心线程数时，创建线程。
  - 当线程数大于等于核心线程数，且任务队列未满时，将任务放入任务队列。
  - 当线程数大于等于核心线程数，且任务队列已满
    1. 若线程数小于最大线程数，创建线程
    2. 若线程数等于最大线程数，抛出异常，拒绝任务

# 39.hive 如何转mr
1. 解析HiveQL查询
- Hive接收HiveQL查询并使用解析器将其转换为抽象语法树（AST）。解析器负责识别查询中的关键字、表名、列名等.
2. 优化查询计划
- Hive使用查询优化器对抽象语法树进行优化。优化器会尝试重新安排查询计划以提高性能，例如选择更合适的连接算法或使用索引。
3. 转换为逻辑计划
- 优化后的查询计划被转换为逻辑计划。逻辑计划是一种表示查询逻辑的抽象结构，它不依赖于具体的执行引擎。
4. 转换为物理计划
- 逻辑计划被进一步转换为物理计划。物理计划是一种依赖于具体执行引擎的计划表示。
5. 生成MapReduce作业
- 物理计划被翻译为一系列MapReduce作业。每个作业执行一个或多个MapReduce任务来处理数据。
# 40.flink 的watermark  shardGroup的概念
在flink当中，当我们基于event time 进行窗口计算时，由于数据存在乱序和延迟到来的问题，即最先进入窗口计算的数据不一定是在业务上最先产生的数据，所以我们需要提供一种机制，保证对应窗口内的数据已经到达，这样才能触发窗口计算，这个机制就是watermark机制。

watermark是flink为了处理event time窗口计算提出的一种机制，本质上就是一个时间戳，代表着比这个时间早的事件已经全部进入到相应的窗口，后续不会在有比这个时间小的事件出现，基于这个前提我们才有可能将event time窗口视为完整并触发窗口的计算。

我们知道，流处理从事件产生，到流经source，再到operator，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序出现，但是也不排除由于网络延迟等原因，导致乱序的产生，特别是使用kafka的话，多个分区的数据无法保证有序。所以在进行window计算的时候，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了。这个特别的机制，就是watermark，watermark是用于处理乱序事件的，watermark可以翻译为水位线！

注意：基于event time窗口计算，存在一个问题就是延迟和乱序，才提出了watermark。

在 Flink 中，ShardGroup 是通过 Key-Group 进行管理的。Key-Group 是将数据按照 key 进行分组的一种机制。每个 Key-Group 包含一个或多个分片，而每个分片都包含一个或多个数据记录。

通过 ShardGroup，Flink 可以将数据分发到不同的任务实例上进行并行处理。每个任务实例负责处理一个或多个 ShardGroup，从而充分利用集群资源。
# 41.jvm常见的垃圾回收算法
- 标记-清除算法（Mark and Sweep）：首先，从根对象开始标记所有可达的对象，然后清除未标记的对象，并回收内存。该算法的缺点是会产生内存碎片。
- 标记-复制算法（Mark and Copy）：将堆内存分为两个区域，一半是活动对象，另一半是空闲的。首先，从根对象开始标记所有可达的对象，然后将存活的对象复制到空闲区域，并清除旧区域。该算法的优点是减少了内存碎片，但代价是浪费了一部分内存空间。
- 标记-整理算法（Mark and Compact）：将堆内存分为两个区域，一个是活动对象，另一个是空闲的。首先，从根对象开始标记所有可达的对象，然后将存活的对象向一端移动，最后清除边界以外的内存。该算法的优点是减少了内存碎片，但代价是需要移动存活的对象。
- 分代收集算法（Generational Collection）：基于对象生命周期的观察结果，将堆内存分为几代。通常将新创建的对象放在年轻代，而长时间存活的对象则放在老年代。不同代使用不同的垃圾回收算法，如年轻代使用复制算法，老年代使用标记-清除或标记-整理算法。这样可以根据对象的特性进行更精细的垃圾回收，提高回收效率。
